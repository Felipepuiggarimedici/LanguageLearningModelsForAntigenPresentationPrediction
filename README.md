Thesis in progress. Its a Fine Tuned model for the TransMut from paper below.

## Citation

This work is based on the Transformer model from:

Yanyi Chu, Yan Zhang, Qiankun Wang, Lingfeng Zhang, Xuhong Wang, Yanjing Wang, Dennis Russell Salahub, Qin Xu, Jianmin Wang, Xuanlu Jiang, Yi Xiong, Dongqing Wei,  
"A transformer-based model to predict peptide–HLA class I binding and optimize mutated peptides for vaccine design,"  
*Nature Machine Intelligence*, vol. 4, pp. 300–311, 2022.  
[https://api.semanticscholar.org/CorpusID:247634099](https://api.semanticscholar.org/CorpusID:247634099)

Code at https://github.com/a96123155/TransPHLA-AOMP 

and the AntiBERTa framework described by:

Jinwoo Leem, Laura S. Mitchell, James H.R. Farmery, Justin Barton, Jacob D. Galson,
"Deciphering the language of antibodies using self-supervised learning,"
Patterns, vol. 3, no. 7, 2022.
https://doi.org/10.1016/j.patter.2022.100513

Code at https://github.com/alchemab/antiberta/blob/master/mlm.ipynb